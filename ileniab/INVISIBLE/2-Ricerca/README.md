# Mancanza del contatto fisico

Vorrei rendere visibile la presenza dell’altro utilizzando il senso del tatto, per permettere all'utente distogliersi dallo schermo e ritagliarsi un momento più intimo e personale. 

La mia attenzione si concentra sulla tattilità, la quale non è visibile o condivisibile, se si è distanti. Sarebbe quindi interessante riuscire ad avvicinare le persone lontane dando fisicità ai gesti e alle emozioni.

Come rendere però presente e tangibile una persona lontana? 

**Quali dati raccolgo:**

Fisici con un sensore tattile?
Movimento delle mani?

**Come li trasformo:**

In vibrazione?
Superficie in rilievo?

## Reference

### Cos’è la tecnologia aptica

Descrive interfacce che forniscono un **feedback tattile** a un utente, come le **vibrazioni**.
Il sistema aptico gestisce la sfera del contatto: chi tocchiamo e con quanta frequenza e anche dove testimoniamo il tipo di relazione che abbiamo con qualcuno. 

I dispositivi tattili possono essere raggruppati in tre tipi principali: **afferrabili, indossabili e toccabili.**

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-1.png)

[**ULTRALEAP**](https://www.ultraleap.com/),  è un Insieme di altoparlanti ad **ultrasuoni** che possono essere controllati individualmente. Essi vengono attivati con differenze di tempo molto specifiche, il luogo in cui tutte le onde ultrasoniche coincidono viene chiamato punto focale.

Utilizziando un dispositivo di tracciamento della mano (Leap Motion Controller) si traccia l'esatta posizione del punto focale e la mano riconoscerà la pressione.

### Cos’è la Laep motion
La Leap Motion è una startup, è specializzata nel fabbricare tecnologie di **rilevazione del movimento** nell'interazione fra uomo e computer.

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-2.png)

Il **Leap Motion Controller** è un modulo di tracciamento ottico della mano che cattura i movimenti delle mani con una precisione senza pari.
Rende l'interazione umana nei mondi digitali naturale.

### Hand tracking

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-3.png)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-4.png)

### [MediaPipe](https://mediapipe.readthedocs.io/en/latest/hand_tracking_desktop.html#tensorflow-lite-hand-tracking-demo-with-webcam-cpu)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-5.png)

### [Pulse](https://www.behance.net/gallery/34196591/PULSE-Tangible-Emotion-Translator?tracking_source=search_projects_recommended%7Cvibration)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-6.png)

Utilizzando delle strutture superficiali, **codifica le emozioni tramite aspetti tattili e vibrazioni**.
Agendo su **forme geometriche** in cui le emozioni positive sono legate a forme rotonde ed le emozioni negative a forme taglienti e angolari. Ognuna delle nostre piastre emozionali è stata dotata di un tag RFID in modo che la **frequenza corretta** possa essere riprodotta quando viene posizionata sulla scatola.

### [Synchrony](http://kennethtay.com/synchrony)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-7.png)

è uno strumento terapeutico progettato per aiutare genitori e bambini con autismo a sviluppare l'intimità e promuovere la comprensione reciproca attraverso un **gioco musicale** improvvisato. Utilizza una forma simile ad un tamburo, la **superficie** è in silicone e **sensibile al tatto**, producendo volume e risonanza in base alla pressione e alla velocità applicate.

### [Physicality](http://mono-grid.com/en/project/physicality/)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-8.png)

L’installazione **registra i dati** degli oggetti che si toccano e li **riproduce** tramite **vibrazioni e variazione di temperatura**. Associano la sensazione tattile con una proiezione grafica.

### [Feel the View](http://www.aedoproject.eu/aedo-project/)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-9.png)

Il dispositivo si applica direttamente al finestrino, una fotocamera scatta una foto, che si trasforma in una sorta di **mappa tattile**. Il dispositivo identifica la precisa posizione del dito, e ritornare la corretta vibrazione. In questo modo si potrà esplorare l’immagine tramite la vibrazione.

### [inFORM](https://www.fastcompany.com/3021522/mit-invents-a-shapeshifting-display-you-can-reach-through-and-touch)

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-10.png)

Si tratta di un Pinscreen, ognuno di questi "pin" è collegato a un motore controllato da un portatile nelle vicinanze, che non solo può spostare i pin per **rendere fisicamente il contenuto digitale**, ma può anche registrare oggetti reali che interagiscono con la sua superficie grazie ai sensori di un Microsoft Kinect.
__________________________________________________________________________________________________________


# Emozioni velate

Il viso coperto dalle mascherine crea uno spazio su cui lavorare. 
Come è possibile rendere visibile la nostra espressione in un altro modo?

**Come:**
Riconoscimento facciale, riconoscimento spazio chiaro (mascherina).  
Riconoscimento espressione, che modificherà l’ouput.  
Utilizzare la Realtà Aumentata.  

**Una tela bianca:**
Ricostruire il volto?  
Mimare il labiale?  
Un camminatore che scopre un disegno nascosto, il camminatore può essere controllato?  

## Reference

### Rilevazione o riconoscimento facciale

![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-11.png)

La rilevazione facciale si riferisce alla capacità di **rilevare quando un volto è presente in un'immagine**. Il riconoscimento facciale si basa sul rilevamento facciale per stabilire se un volto è presente in un'immagine ma fa un ulteriore passo avanti e tenta di **stabilire di chi sia il volto**.

### [Emotion recognition](https://github.com/oarriaga/face_classification?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-12.png)

### [Face Api](https://learn.ml5js.org/docs/#/reference/face-api)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-13.png)

### [Speech2Face](https://speech2face.github.io/)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-14.png)
Ricostruisce un'**immagine facciale** di una persona da una breve** registrazione audio** di quella persona che parla. Utilizzano il deep learning per eseguire questo compito, usando milioni di video naturali di persone che parlano da Internet / Youtube. Durante l'addestramento, il modello apprende correlazioni audiovisive con la voce. Successivamente produce immagini che catturano vari **attributi fisici** degli oratori come **età, genere ed etnia.**

### [Deep Dream](https://towardsdatascience.com/lets-read-a-story-a-study-on-storytelling-for-children-using-machine-learning-tools-1b631bbbffac)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-15.png)
Deep Dream è un programma di elaborazione delle immagini scritto da Google. Utilizza una rete neurale per **trovare e potenziare dei pattern all'interno di immagini** tramite algoritmi che creano effetti allucinogeni che richiamano le sembianze di un sogno.

### [Proiettore facciale indossabile](http://jingcailiu.com/?portfolio=wearable-face-projector)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-16.png)
Questo è uno dei progetti realizzati dagli studenti nel 2017 (Marcel Coufrer, Sanne Weekers, Joppe Besseling, Jip van Leeuwenstein e Jing-cai Liu) dell'Università delle Arti di Utrecht.

### [La maschera che inganna](https://github.com/BruceMacD/Adversarial-Faces)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-17.png)
Un disegno come un **falso positivo** che inganna la rilevazione facciale, in modo tale che la stampa su una maschera apparisse poco appariscente.

### [Lettura labiale](https://medium.com/mlreview/multi-modal-methods-part-one-49361832bc7e)
![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-18.png)
1. Crea esempi
2. Estrae delle labbra le caratteristiche
3. Approfondimento
4. Collegamento LipNet a livello di frase  

**LipNet** si avvale anche di un algoritmo aggiuntivo  utilizzato nei sistemi di riconoscimento vocale - Classificazione Temporale Connessionista (CTC).
dell'uscita GRU è elaborato da uno strato lineare e da un softmax. Questo modello end-to-end è addestrato con CTC.


### [Ricreare il movimento](https://towardsdatascience.com/ai-generated-elon-musk-joined-a-zoom-call-has-gone-viral-c0516e99a37c)
I ricercatori di machine learning di Samsung hanno prodotto un sistema in grado di ricreare il movimento realistico da un solo fotogramma del volto di una persona, aprendo la possibilità di animare non solo foto ma anche dipinti.
 ![foto](https://github.com/ileniab/archive/blob/master/ileniab/INVISIBLE/2.Ricerca/img/img-19.png)
**Avatarify** è abbastanza robusto da funzionare in tempo reale durante lo streaming.  L'autore importa una foto e la persona nella foto viene animata in base al movimento dell'autore stesso che si muove davanti a una telecamera. Alzare le sopracciglia, sbattere le palpebre e parlare in conferenza in tempo reale.
